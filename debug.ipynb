{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import random\n",
    "import os \n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class CFG():\n",
    "    model_name = \"binhquoc/vie-deberta-small\"\n",
    "    lr = 3e-4\n",
    "    weight_decay = 0.01\n",
    "    dropout = 0.1\n",
    "    lamda = 0.9\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    epoch = 3\n",
    "    seed = 42\n",
    "    train_batch_size = 32\n",
    "    eval_batch_size = 64\n",
    "    device= torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    max_length = 512,\n",
    "\n",
    "cfg = CFG()\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_everything(seed=cfg.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def read_json(path):\n",
    "    with open(path, \"r\", encoding='utf-8') as f:\n",
    "        json_data = json.load(f)\n",
    "    return json_data\n",
    "\n",
    "train_json = read_json('data/train.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_label = open('data/intent_label.txt', 'r').readlines()\n",
    "intent_label = [i.replace('\\n', '') for i in intent_label]\n",
    "\n",
    "slot_label = open('data/slot_label.txt', 'r').readlines()\n",
    "slot_label = [i.replace('\\n', '') for i in slot_label]\n",
    "\n",
    "cfg.intent2id = {intent:idx for idx, intent in enumerate(intent_label)}\n",
    "cfg.slot2id = {slot:idx for idx, slot in enumerate(slot_label)}\n",
    "cfg.id2intent = {idx:intent for idx, intent in enumerate(intent_label)}\n",
    "cfg.id2slot = {idx:slot for idx, slot in enumerate(slot_label)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intent: schedule\n",
      "bạn cho cô Tháng hay lịch vào ngày   chín   âm     tháng   rồi     \n",
      "O   O   O  O     O   O    O   B-date I-date I-date B-month I-month \n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "temp_data = train_json[index]\n",
    "print('intent: ' + temp_data[\"intent\"])\n",
    "tokens = temp_data[\"text\"].split()\n",
    "slot_tags = temp_data[\"slot\"].split()\n",
    "\n",
    "line1 = \"\"\n",
    "line2 = \"\"\n",
    "for word, label in zip(tokens, slot_tags):\n",
    "    max_length = max(len(word), len(label))\n",
    "    line1 += word + \" \" * (max_length - len(word) + 1)\n",
    "    line2 += label + \" \" * (max_length - len(label) + 1)\n",
    "\n",
    "print(line1)\n",
    "print(line2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'B-round_trip'.startswith('B-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"binhquoc/vie-deberta-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '▁bạn', '▁cho', '▁cô', '▁Tháng', '▁hay', '▁lịch', '▁vào', '▁ngày', '▁chín', '▁âm', '▁tháng', '▁rồi', '[SEP]']\n",
      "[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, None]\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(tokens, is_split_into_words=True)\n",
    "print(inputs.tokens())\n",
    "print(inputs.word_ids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id]\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            label_name = cfg.id2slot[label]\n",
    "            if label_name.startswith('B-'):\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels\n",
    "\n",
    "def tags2labels(tags, labels_mapping):\n",
    "    return [labels_mapping[tag] for tag in tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-100, 0, 0, 0, 0, 0, 0, 0, 10, 11, 11, 14, 15, -100]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slot_tags_labels = tags2labels(slot_tags , cfg.slot2id)\n",
    "align_labels_with_tokens(slot_tags_labels, inputs.word_ids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class WorldCupDataset(Dataset):\n",
    "    def __init__(self, cfg, data_json, is_train=False):\n",
    "        super().__init__()\n",
    "        # if cfg.model_name in ['vinai/phobert-base', 'vinai/phobert-large']:\n",
    "        #     segmentator = Segmentation()\n",
    "        #     data_json = [merge_slot_after_segmentation(item, segmentator) for item in data_json]\n",
    "            \n",
    "        self.slot2id = cfg.slot2id\n",
    "        self.intent2id = cfg.intent2id\n",
    "        self.tokenizer = cfg.tokenizer\n",
    "        self.cfg = cfg\n",
    "        self.is_train = is_train\n",
    "        self.tokenized_data = [self.process_data(data) for data in data_json]\n",
    "        \n",
    "    def  __len__(self):\n",
    "        return len(self.tokenized_data)\n",
    "\n",
    "    def process_data(self, data):\n",
    "        inputs = self.tokenizer(data['text'])\n",
    "        if self.is_train:\n",
    "            if self.tokenizer.is_fast:\n",
    "                intent_label = self.intent2id[data['intent']] if data['intent'] in self.intent2id else self.intent_dict['UNK']\n",
    "                slot_tags = data[\"slot\"].split()\n",
    "                slot_labels = tags2labels(slot_tags , self.slot2id)\n",
    "                new_slot_labels = align_labels_with_tokens(slot_labels, inputs.word_ids())\n",
    "                inputs['intent_label'] = intent_label\n",
    "                inputs['slot_labels'] = new_slot_labels\n",
    "            else:\n",
    "                raise('Non-fast tokenizer not processed yet')\n",
    "        del inputs['token_type_ids']\n",
    "        return inputs\n",
    "    \n",
    "    def align_labels_with_tokens(labels, word_ids):\n",
    "        new_labels = []\n",
    "        current_word = None\n",
    "        for word_id in word_ids:\n",
    "            if word_id != current_word:\n",
    "                # Start of a new word!\n",
    "                current_word = word_id\n",
    "                label = -100 if word_id is None else labels[word_id]\n",
    "                new_labels.append(label)\n",
    "            elif word_id is None:\n",
    "                # Special token\n",
    "                new_labels.append(-100)\n",
    "            else:\n",
    "                # Same word as previous token\n",
    "                label = labels[word_id]\n",
    "                # If the label is B-XXX we change it to I-XXX\n",
    "                label_name = cfg.id2slot[label]\n",
    "                if label_name.startswith('B-'):\n",
    "                    label += 1\n",
    "                new_labels.append(label)\n",
    "\n",
    "        return new_labels\n",
    "\n",
    "    def tags2labels(tags, labels_mapping):\n",
    "        return [labels_mapping[tag] if tag in labels_mapping else labels_mapping['UNK'] for tag in tags]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.tokenized_data[idx]\n",
    "    \n",
    "################################\n",
    "wc_train_dataset = WorldCupDataset(cfg, train_json[:int(0.8*len(train_json))], True)\n",
    "wc_eval_dataset = WorldCupDataset(cfg, train_json[int(0.8*len(train_json)):], True)\n",
    "# wc_test_dataset = WorldCupDataset(cfg, train_json, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [5, 126, 17, 211, 1750, 168, 438, 37, 59, 2296, 730, 136, 259, 4], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'intent_label': 2, 'slot_labels': [-100, 0, 0, 0, 0, 0, 0, 0, 10, 11, 11, 14, 15, -100]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wc_train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Collate:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        output = {k:[sample[k] for sample in batch] for k in batch[0].keys()}\n",
    "\n",
    "        # calculate max token length of this batch\n",
    "        batch_max = max([len(ids) for ids in output[\"input_ids\"]])\n",
    "\n",
    "        # add padding\n",
    "        output[\"input_ids\"] = [s + (batch_max - len(s)) * [self.tokenizer.pad_token_id] for s in output[\"input_ids\"]]\n",
    "        output[\"attention_mask\"] = [s + (batch_max - len(s)) * [0] for s in output[\"attention_mask\"]]\n",
    "        if \"slot_labels\" in output:\n",
    "            output[\"slot_labels\"] = [s + (batch_max - len(s)) * [-100] for s in output[\"slot_labels\"]]\n",
    "\n",
    "        # convert to tensors\n",
    "        output = {k:torch.tensor(v, dtype=torch.long) for k,v in output.items()}\n",
    "\n",
    "        return output\n",
    "\n",
    "collate_fn = Collate(cfg.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    5,   126,    17,   211,  1750,   168,   438,    37,    59,  2296,\n",
       "            730,   136,   259,     4,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0],\n",
       "         [    5,  1194,    17,  1983,   347,  3489,   109,   140,   205,   263,\n",
       "              9,   313,   197,   704, 13222,  3024,    59,  2669,  2338,    43,\n",
       "            171,     4]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),\n",
       " 'intent_label': tensor([2, 5]),\n",
       " 'slot_labels': tensor([[-100,    0,    0,    0,    0,    0,    0,    0,   10,   11,   11,   14,\n",
       "            15, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
       "         [-100,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    2,    3,   10,   11,   11,    0,    0, -100]])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_collator = collate_fn(wc_train_dataset[0:2])\n",
    "out_collator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"Applies attention mechanism on the `context` using the `query`.\n",
    "    Args:\n",
    "        dimensions (int): Dimensionality of the query and context.\n",
    "        attention_type (str, optional): How to compute the attention score:\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size = 256):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, query, context, attention_mask):\n",
    "        if not (query.size()[-1] == self.hidden_size and context.size()[-1] == self.hidden_size):\n",
    "            raise Exception(\"Dimensions must be equal\")\n",
    "\n",
    "        attention_scores = torch.bmm(query, context.transpose(1, 2).contiguous())\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = torch.unsqueeze(attention_mask, 2)\n",
    "            attention_scores.masked_fill_(attention_mask == 0, - np.inf)\n",
    "\n",
    "        attention_weights = self.softmax(attention_scores)\n",
    "        context_vector = torch.bmm(attention_weights.transpose(1, 2), query)\n",
    "        combined = torch.add(context_vector, context)\n",
    "        output = self.tanh(combined)\n",
    "\n",
    "        return output, attention_weights\n",
    "\n",
    "class IntentClassifier(nn.Module):\n",
    "    def __init__(self, input_dim = 256, num_intent_labels = 25, dropout_rate=0.0):\n",
    "        super(IntentClassifier, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.linear = nn.Linear(input_dim, num_intent_labels)\n",
    "        self.__init_fc_params()\n",
    "    \n",
    "    def __init_fc_params(self):\n",
    "        nn.init.xavier_normal_(self.linear.weight)\n",
    "        nn.init.constant_(self.linear.bias, 0)\n",
    "\n",
    "    def forward(self, intent_input):\n",
    "        intent_input = self.dropout(intent_input)\n",
    "\n",
    "        return self.linear(intent_input)\n",
    "\n",
    "\n",
    "class SlotClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim = 768,\n",
    "        num_slot_labels = 142,\n",
    "        attention_embedding_size = 256,\n",
    "        dropout_rate = 0.0,\n",
    "    ):\n",
    "        super(SlotClassifier, self).__init__()\n",
    "\n",
    "        self.attention = Attention(attention_embedding_size)\n",
    "        self.linear_slot = nn.Linear(input_dim, attention_embedding_size)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.linear_out = nn.Linear(attention_embedding_size, num_slot_labels)\n",
    "        self.__init_fc_params()\n",
    "    \n",
    "    def __init_fc_params(self):\n",
    "        nn.init.xavier_normal_(self.linear_slot.weight)\n",
    "        nn.init.constant_(self.linear_slot.bias, 0)\n",
    "\n",
    "        nn.init.xavier_normal_(self.linear_out.weight)\n",
    "        nn.init.constant_(self.linear_out.bias, 0)\n",
    "\n",
    "    def forward(self, sequence_output, intent_context, attention_mask=None):\n",
    "        sequence_output = self.linear_slot(sequence_output)         # (b, seq, hidden_size) -> (b, seq, att_size)\n",
    "        intent_context = self.linear_slot(intent_context)           # (b, hidden_size) -> (b, att_size)\n",
    "        intent_context = torch.unsqueeze(intent_context, 1)         # (b, att_size) -> (b, 1, att_size)\n",
    "        \n",
    "        attention_output, weights = self.attention(sequence_output, intent_context, attention_mask)     #(b, 1, att_size)\n",
    "        intent_input = attention_output.squeeze(1)                  # (b, 1, att_size) -> #(b, att_size)\n",
    "        sequence_output = torch.mul(sequence_output, attention_output)      #(b, seq, att_size)\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "\n",
    "        return intent_input, self.linear_out(sequence_output)\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     slot = SlotClassifier(num_slot_labels=len(cfg.slot2id))\n",
    "#     intent = IntentClassifier(num_intent_labels =len(cfg.intent2id))\n",
    "\n",
    "#     sequence_output = torch.rand((2,50,768))\n",
    "#     pooled_output = torch.rand((2, 768))\n",
    "#     tmp_attention_mask = None\n",
    "\n",
    "#     intent_in, slot_out = slot(sequence_output=sequence_output, intent_context=pooled_output, attention_mask=tmp_attention_mask)\n",
    "#     intent_out = intent(intent_input=intent_in)\n",
    "\n",
    "#     print(\"Intent_in size: \", intent_in.size())\n",
    "#     print(\"Intent_out size: \", intent_out.size())\n",
    "#     print(\"Slot_out size: \", slot_out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at binhquoc/vie-deberta-small were not used when initializing DebertaV2Model: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoConfig\n",
    "class WorldCup(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super(WorldCup, self).__init__()\n",
    "        self.model = AutoModel.from_pretrained(cfg.model_name)\n",
    "        self.config_model = AutoConfig.from_pretrained(cfg.model_name)\n",
    "        self.num_slot_labels = len(cfg.slot2id)\n",
    "        self.num_intent_labels = len(cfg.intent2id)\n",
    "        self.slot = SlotClassifier(input_dim = self.config_model.hidden_size, num_slot_labels = self.num_slot_labels, dropout_rate = cfg.dropout)\n",
    "        self.intent = IntentClassifier(num_intent_labels = self.num_intent_labels, dropout_rate=cfg.dropout)\n",
    "        self.loss_function = nn.CrossEntropyLoss(ignore_index=- 100)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, intent_label = None, slot_labels = None):\n",
    "        last_hidden_state = self.model(input_ids, attention_mask).last_hidden_state\n",
    "        pooled_cls = last_hidden_state[:,0,:]\n",
    "\n",
    "        # JointIDSF ver 1\n",
    "        intent_in, slot_out = self.slot(sequence_output=last_hidden_state, intent_context=pooled_cls, attention_mask=attention_mask)\n",
    "        intent_out = self.intent(intent_input=intent_in)\n",
    "  \n",
    "        if intent_label != None and slot_labels != None:\n",
    "            #Cal loss\n",
    "            intent_loss = self.loss_function(intent_out, intent_label)\n",
    "            slot_loss = self.loss_function(slot_out.view(-1, self.num_slot_labels), slot_labels.view(-1))\n",
    "            return {\n",
    "                'intent_loss': intent_loss,\n",
    "                'slot_loss': slot_loss, \n",
    "                'intent_logits': intent_out, \n",
    "                'slot_logits': slot_out\n",
    "            }\n",
    "        else:\n",
    "            # just return logits\n",
    "            return {\n",
    "                'intent_logits': intent_out, \n",
    "                'slot_logits': slot_out\n",
    "            }\n",
    "\n",
    "worldcup_model = WorldCup(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# worldcup_model(**out_collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_loader = DataLoader(wc_train_dataset, batch_size = cfg.train_batch_size, collate_fn = collate_fn, shuffle=True, pin_memory=True)\n",
    "eval_loader = DataLoader(wc_eval_dataset, batch_size = cfg.eval_batch_size, collate_fn = collate_fn, pin_memory=True, drop_last= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer and Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "\n",
    "optimizer = AdamW(worldcup_model.parameters(), lr=cfg.lr)\n",
    "num_update_steps_per_epoch = len(train_loader)\n",
    "num_training_steps = cfg.epoch * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=int(num_training_steps/10),\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Evaluate, Inference function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slot_postprocess(slot_predictions, slot_labels):\n",
    "    predictions = slot_predictions.detach().cpu().clone().numpy()\n",
    "    labels = slot_labels.detach().cpu().clone().numpy()\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[cfg.id2slot[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [cfg.id2slot[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    return true_predictions, true_labels\n",
    "\n",
    "def intent_postprocess(intent_predictions, intent_labels):\n",
    "    predictions = intent_predictions.detach().cpu().clone().numpy()\n",
    "    labels = intent_labels.detach().cpu().clone().numpy()\n",
    "    \n",
    "    true_labels = [cfg.id2intent[l]  for l in labels]\n",
    "    true_predictions = [cfg.id2intent[l]  for l in predictions]\n",
    "    return true_predictions, true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea12e9312624445cadbd8f159e5f96a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/viet/miniconda3/envs/worldcup/lib/python3.9/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:745: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_scores = torch.bmm(query_layer, key_layer.transpose(-1, -2)) / torch.tensor(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: {'slot_eval_loss': 0.003985847205816147, 'intent_eval_loss': 0.003385974205679871, 'slot_accuracy': 0.9992992828598019, 'intent_accuracy': 0.9996639784946236}\n",
      "epoch 1: {'slot_eval_loss': 0.0014737916695711675, 'intent_eval_loss': 0.0003321582275587985, 'slot_accuracy': 0.9997262823671101, 'intent_accuracy': 1.0}\n",
      "epoch 2: {'slot_eval_loss': 0.0005013562409188917, 'intent_eval_loss': 0.0003020500561462775, 'slot_accuracy': 0.9998576668308973, 'intent_accuracy': 1.0}\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'checkpoint/final_checkpoint'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_434702/1267836657.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     69\u001b[0m         },\n\u001b[1;32m     70\u001b[0m     )\n\u001b[0;32m---> 71\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworldcup_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'checkpoint/final_checkpoint'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/worldcup/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0m_check_dill_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/worldcup/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/worldcup/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'checkpoint/final_checkpoint'"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "worldcup_model.to(cfg.device)\n",
    "for epoch in range(cfg.epoch):\n",
    "    # Training\n",
    "    worldcup_model.train()\n",
    "    for batch in train_loader:\n",
    "        batch = {k:v.to(cfg.device, dtype = torch.long) for k,v in batch.items()}\n",
    "        outputs = worldcup_model(**batch)\n",
    "        loss = outputs['slot_loss']*cfg.lamda + outputs['intent_loss']*(1-cfg.lamda)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.set_postfix(\n",
    "            Epoch = epoch,\n",
    "            slot_loss = outputs['slot_loss'].item(),\n",
    "            intent_loss = outputs['intent_loss'].item(),\n",
    "            combined_loss = loss.item(),\n",
    "            lr = optimizer.param_groups[0]['lr']\n",
    "        )\n",
    "        progress_bar.update(1)\n",
    "    # Evaluation\n",
    "    worldcup_model.eval()\n",
    "    total_slot_eval_loss = 0\n",
    "    total_intent_eval_loss = 0\n",
    "    slot_predictions_list = []\n",
    "    slot_labels_list = []\n",
    "    intent_predictions_list = []\n",
    "    intent_labels_list = []\n",
    "\n",
    "    for batch in eval_loader:\n",
    "        with torch.no_grad():\n",
    "            batch = {k:v.to(cfg.device, dtype = torch.long) for k,v in batch.items()}\n",
    "            outputs = worldcup_model(**batch)\n",
    "        \n",
    "        total_slot_eval_loss += outputs['slot_loss'].item()\n",
    "        total_intent_eval_loss += outputs['intent_loss'].item()\n",
    "\n",
    "        intent_predictions = outputs['intent_logits'].argmax(dim=-1)\n",
    "        slot_predictions = outputs['slot_logits'].argmax(dim=-1)\n",
    "        \n",
    "        # Convert index to label name\n",
    "        slot_predictions, slot_labels = slot_postprocess(slot_predictions, batch['slot_labels'])\n",
    "        intent_predictions, intent_labels = intent_postprocess(intent_predictions, batch['intent_label'])\n",
    "        \n",
    "\n",
    "        # Compute metric (accuracy)\n",
    "        slot_predictions_list += slot_predictions\n",
    "        slot_labels_list += slot_labels\n",
    "        intent_predictions_list += intent_predictions\n",
    "        intent_labels_list += intent_labels\n",
    "    \n",
    "    # flatten results\n",
    "    slot_predictions_list = [slot for slots in slot_predictions_list for slot in slots]\n",
    "    slot_labels_list = [slot for slots in slot_labels_list for slot in slots]\n",
    "\n",
    "    slot_accuracy = sum([(1 if p==l else 0)for p,l in zip(slot_predictions_list, slot_labels_list)])/len(slot_predictions_list)\n",
    "    intent_accuracy = sum([(1 if p==l else 0)for p,l in zip(intent_predictions_list, intent_labels_list)])/len(intent_predictions_list)\n",
    "    print(\n",
    "        f\"epoch {epoch}:\",\n",
    "        {\n",
    "            'slot_eval_loss': total_slot_eval_loss/len(eval_loader),\n",
    "            'intent_eval_loss': total_intent_eval_loss/len(eval_loader),\n",
    "            'slot_accuracy':slot_accuracy,\n",
    "            'intent_accuracy':intent_accuracy\n",
    "        },\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('checkpoints/tokenizer_config.json',\n",
       " 'checkpoints/special_tokens_map.json',\n",
       " 'checkpoints/tokenizer.json')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(worldcup_model.state_dict(), 'checkpoints/final_checkpoint.bin')\n",
    "tokenizer.save_pretrained('checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.3577, -3.0024, -1.6577, -0.9406, -3.8050, -0.9548, -0.5570,  7.1578]])\n",
      "tensor([[[ 5.5049e+00,  2.8271e-01,  8.3822e+00,  3.8825e+00, -2.6597e+00,\n",
      "          -9.9495e-01, -2.4784e-01,  1.1292e+00,  7.4237e-01, -1.8747e+00,\n",
      "          -1.8218e+00,  1.3861e+00, -1.8063e+00,  3.1873e-01, -3.2052e+00,\n",
      "           1.7141e+00, -4.8388e-01, -1.0889e+00, -1.2534e+00, -1.0277e-01,\n",
      "          -7.3484e-01, -1.3350e+00,  8.6325e-01, -8.3508e-01, -3.0208e-02,\n",
      "           1.1886e+00],\n",
      "         [ 1.5911e+01, -1.5504e+00,  3.0659e+00,  1.7730e+00, -6.1234e-01,\n",
      "          -4.6469e-01,  8.3353e-01,  1.1889e+00, -2.6595e+00, -8.2903e-01,\n",
      "          -4.5051e-01, -4.8398e-01, -2.5500e+00, -2.6374e+00, -1.2673e+00,\n",
      "           6.4588e-01, -9.1391e-01, -3.6846e+00,  1.8230e+00, -5.7142e-02,\n",
      "          -1.8725e+00, -5.9644e-01,  2.0268e+00,  6.2443e-02,  5.4360e-01,\n",
      "          -2.6621e-01],\n",
      "         [ 1.7715e+01, -1.6150e+00,  1.7190e+00,  2.1890e+00, -1.5864e+00,\n",
      "          -1.0598e+00,  4.6863e-01,  4.9405e-02, -1.8001e+00, -9.7388e-01,\n",
      "           3.8331e-01,  8.7854e-02, -4.2806e+00, -2.1646e+00,  3.3664e-01,\n",
      "           1.1843e+00, -1.6378e+00, -3.6946e+00,  2.6067e+00,  6.9570e-01,\n",
      "          -1.8653e+00, -1.7447e+00,  2.1473e+00, -5.5921e-01,  6.6843e-01,\n",
      "          -3.3405e-01],\n",
      "         [ 1.7389e+01, -1.5188e+00,  2.6408e+00,  2.7711e+00, -1.9615e+00,\n",
      "          -1.1034e+00, -2.6139e-01, -5.1462e-01, -8.0656e-01, -2.1935e+00,\n",
      "          -2.0605e+00, -2.1172e+00, -4.5307e+00, -2.2265e+00,  4.8268e-01,\n",
      "           1.3517e+00, -2.1466e+00, -4.4270e+00,  2.8382e+00, -7.1418e-01,\n",
      "          -8.6293e-01, -2.0732e+00,  8.9934e-01, -3.2211e-01,  1.8083e+00,\n",
      "          -2.5212e-01],\n",
      "         [ 1.3787e+00,  2.2275e+00,  1.4652e+01,  4.2878e+00, -1.4192e-02,\n",
      "          -7.8029e-01,  9.2913e-01, -1.7348e+00,  2.0552e+00, -4.5038e+00,\n",
      "          -2.1152e+00, -5.7056e-01,  2.1551e+00,  3.9163e-01, -1.8558e+00,\n",
      "           4.9930e-01,  1.0811e-01,  3.5830e-01, -1.0870e+00, -2.0788e+00,\n",
      "          -9.3630e-01,  2.1122e+00,  8.1530e-01, -9.2087e-01, -1.6080e+00,\n",
      "           3.8811e+00],\n",
      "         [ 1.3028e+00, -3.4546e+00,  1.4367e+00,  1.6898e+01, -4.4150e+00,\n",
      "          -3.4822e+00,  1.3921e+00,  1.3918e+00,  1.6881e+00,  7.9871e-01,\n",
      "          -6.2644e-01,  3.9647e-01, -3.0399e-01,  1.4490e+00, -3.9283e+00,\n",
      "           9.6258e-01, -2.0434e+00, -1.0672e+00,  1.6534e+00, -1.3923e+00,\n",
      "           8.6300e-01,  2.4854e+00, -2.0877e+00, -1.1222e+00, -4.6371e-01,\n",
      "           5.3620e-02],\n",
      "         [ 2.1221e+00, -2.4761e+00, -2.7768e-01,  1.5063e+01, -5.0543e+00,\n",
      "          -3.6739e+00,  9.2205e-01,  1.0301e+00,  2.6166e-01,  1.7548e+00,\n",
      "          -5.1791e-01, -1.2041e-01, -1.1921e+00,  1.3417e+00, -4.1619e+00,\n",
      "           1.2690e+00, -8.6305e-01, -1.2267e-01,  1.1566e+00, -6.8132e-01,\n",
      "           7.7434e-02,  1.0543e-01, -2.4591e+00, -1.4148e+00,  8.6271e-01,\n",
      "          -1.9748e-01],\n",
      "         [ 7.7653e+00,  1.0600e+00,  3.4645e+00,  7.9943e+00, -1.2503e+00,\n",
      "          -1.9396e+00,  4.5802e+00,  5.9450e-01,  1.2506e+00,  3.9164e-01,\n",
      "          -1.9839e+00, -4.2278e+00, -8.1035e-01,  6.0489e-01, -2.1682e+00,\n",
      "          -1.4780e+00, -2.3272e+00, -2.7972e+00,  2.6684e+00,  6.1093e-01,\n",
      "          -1.6913e-01, -7.7958e-01,  4.0597e-01, -1.9383e+00,  2.0318e+00,\n",
      "           8.0847e-01],\n",
      "         [ 4.8575e+00, -2.3869e+00, -3.5102e+00,  6.3492e+00, -5.0376e-01,\n",
      "          -2.0010e+00,  1.8283e+00,  4.7473e+00,  1.0145e+00,  1.1580e+00,\n",
      "           1.7719e+00, -1.8116e+00, -3.3011e+00,  2.1950e-02, -7.3297e-02,\n",
      "          -1.2017e+00, -2.0022e+00, -2.9075e+00,  4.2116e+00,  7.3324e+00,\n",
      "          -2.5174e-01,  4.2598e-01, -1.1027e+00, -1.0511e+00,  2.5583e+00,\n",
      "           1.7587e+00],\n",
      "         [ 1.6525e+01, -2.0895e+00, -4.4008e-01,  4.3066e+00, -1.6373e+00,\n",
      "          -1.2360e+00,  1.5613e+00,  2.0186e+00, -2.5698e+00, -1.0172e+00,\n",
      "           9.4939e-01, -1.0236e+00, -5.5705e+00, -2.5844e+00, -6.8150e-01,\n",
      "           8.5469e-01, -5.2113e-01, -5.0323e+00,  1.1580e+00,  1.7305e+00,\n",
      "          -1.8883e+00, -7.7185e-03, -8.0251e-01, -5.2129e-01,  8.7816e-01,\n",
      "           2.3387e-01],\n",
      "         [ 1.8434e+01, -3.1788e-01,  2.5823e-01,  4.5874e+00, -1.3881e+00,\n",
      "          -1.4388e+00,  2.6973e+00, -1.3573e-01, -2.6659e+00, -5.6321e-01,\n",
      "           4.8258e-01, -2.2735e+00, -3.7276e+00, -1.4306e+00, -8.9246e-01,\n",
      "           9.4914e-01, -6.1417e-01, -3.9005e+00,  4.9459e-01,  1.5218e+00,\n",
      "          -1.2926e+00, -1.0682e+00, -4.3551e-02, -1.5060e+00,  2.3176e+00,\n",
      "          -3.7073e-02],\n",
      "         [ 1.8469e+01, -1.2342e+00,  7.2043e-01,  3.3057e+00, -6.8667e-01,\n",
      "          -1.0950e+00,  9.1565e-01,  1.3419e+00, -2.2897e+00,  5.3961e-01,\n",
      "           6.2932e-01, -2.2624e+00, -5.4418e+00, -9.8150e-02, -5.5243e-01,\n",
      "           1.3199e+00, -1.2208e+00, -4.4491e+00,  5.6780e-01,  4.3226e+00,\n",
      "          -1.1538e+00, -7.0224e-01,  1.0842e+00,  2.4277e-01,  1.1692e+00,\n",
      "          -7.6202e-01],\n",
      "         [ 1.8414e+01,  1.1622e-01,  2.0742e-01,  2.7321e+00, -2.1761e+00,\n",
      "          -2.8410e+00,  3.2564e+00,  8.4389e-01, -7.0621e-01, -3.7853e-01,\n",
      "          -2.0234e+00, -2.3056e+00, -5.6440e+00, -4.7446e-01, -5.9259e-01,\n",
      "           8.1081e-01, -1.1538e+00, -3.9700e+00,  5.3419e-01,  1.1761e+00,\n",
      "          -2.3465e+00, -1.6798e+00, -3.2212e-01, -4.3449e-01,  2.6918e+00,\n",
      "          -2.3723e-01],\n",
      "         [ 1.6866e+01,  4.6529e-01, -5.5855e-01,  2.5192e+00, -2.9365e+00,\n",
      "          -4.9912e-01,  2.6092e+00,  1.1710e+00, -4.0894e-01,  1.9317e+00,\n",
      "          -4.1627e-01, -1.4918e+00, -4.4788e+00, -9.5382e-01, -4.7520e-02,\n",
      "          -4.7113e-01, -8.2723e-01, -4.7111e+00,  9.3370e-01,  1.9515e+00,\n",
      "          -1.9375e+00, -2.1208e+00, -9.1938e-01, -2.8362e-01,  2.7982e+00,\n",
      "          -6.7909e-01],\n",
      "         [ 1.8083e+01,  3.5259e-01,  9.9507e-01,  3.0323e+00, -2.9437e+00,\n",
      "           9.1510e-03,  2.9533e+00, -3.8282e-01, -8.1711e-01,  2.4037e+00,\n",
      "           2.0195e-01, -6.5771e-01, -4.1684e+00,  1.1349e+00, -2.5092e-01,\n",
      "          -4.3209e-01, -1.0340e+00, -4.7847e+00,  3.8236e-01,  1.7044e+00,\n",
      "          -1.2285e+00, -3.1652e+00, -2.9179e-01, -5.9876e-01,  2.6787e+00,\n",
      "          -1.1586e+00],\n",
      "         [ 5.5049e+00,  2.8271e-01,  8.3822e+00,  3.8825e+00, -2.6597e+00,\n",
      "          -9.9495e-01, -2.4784e-01,  1.1292e+00,  7.4237e-01, -1.8747e+00,\n",
      "          -1.8218e+00,  1.3861e+00, -1.8063e+00,  3.1873e-01, -3.2052e+00,\n",
      "           1.7141e+00, -4.8388e-01, -1.0889e+00, -1.2534e+00, -1.0278e-01,\n",
      "          -7.3484e-01, -1.3350e+00,  8.6325e-01, -8.3508e-01, -3.0208e-02,\n",
      "           1.1886e+00]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/viet/miniconda3/envs/worldcup/lib/python3.9/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:745: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_scores = torch.bmm(query_layer, key_layer.transpose(-1, -2)) / torch.tensor(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'intent': {'intent_type': 'result.point', 'score': 0.9985475},\n",
       " 'slot': [{'slot_type': 'team',\n",
       "   'score': 0.9310429,\n",
       "   'slot_text': '',\n",
       "   'start': 0,\n",
       "   'end': 0},\n",
       "  {'slot_type': 'team',\n",
       "   'score': 0.9999249,\n",
       "   'slot_text': 'bồ đào nha hiện',\n",
       "   'start': 13,\n",
       "   'end': 28}],\n",
       " 'text': 'xếp hạng của bồ đào nha hiện đang như thế nào vậy maika'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn.functional import softmax\n",
    "\n",
    "# text = \"Anh với Ý có đá với nhau không\"\n",
    "def inference(text):\n",
    "    inputs = tokenizer(text, return_tensors = 'pt',return_offsets_mapping= True)\n",
    "    del inputs['token_type_ids']\n",
    "    worldcup_model.to('cpu')\n",
    "    with torch.no_grad():\n",
    "        outputs = worldcup_model(inputs['input_ids'], inputs['attention_mask'])\n",
    "\n",
    "    intent_sm = softmax(outputs['intent_logits'], dim=-1)\n",
    "    intent_score, intent_id = torch.max(intent_sm, dim = -1)\n",
    "\n",
    "    score = softmax(outputs['slot_logits'], dim=-1)\n",
    "    slot_score, slot_id= torch.max(score, dim = -1)\n",
    "\n",
    "    intent_score = intent_score.detach().numpy()[0]\n",
    "    slot_score = slot_score.detach().numpy()[0]\n",
    "    intent_id = intent_id.detach().numpy()[0]\n",
    "    slot_id = slot_id.detach().numpy()[0]\n",
    "    # print(slot_id)\n",
    "\n",
    "    intent_label = cfg.id2intent[intent_id]\n",
    "    slot_label = [cfg.id2slot[id] for id in slot_id]\n",
    "\n",
    "    offset_mapping = inputs['offset_mapping'].detach().numpy()[0]\n",
    "    slot_list = []\n",
    "    real_tag = None\n",
    "    for i, tag in enumerate(slot_label):\n",
    "        if tag.startswith(\"B-\") and real_tag is None:\n",
    "            start = offset_mapping[i][0]\n",
    "            start = start + 1 if start != 0 else start # + 1 to offset of token with a space before\n",
    "            real_tag = tag[2:]\n",
    "            score = slot_score[i]\n",
    "        elif tag.startswith(\"I-\") and tag[2:] == real_tag:\n",
    "            continue\n",
    "        elif real_tag is not None:\n",
    "            end = offset_mapping[i-1][1]\n",
    "            slot_list.append({\n",
    "                'slot_type': real_tag,\n",
    "                'score': score,\n",
    "                'slot_text':  text[start:end],\n",
    "                'start': start,\n",
    "                'end': end\n",
    "            })\n",
    "            real_tag = None\n",
    "            if tag.startswith(\"B-\"):\n",
    "                start = offset_mapping[i][0]\n",
    "                start = start + 1 if start != 0 else start # + 1 to offset of token with a space before\n",
    "                real_tag = tag[2:]\n",
    "                score = slot_score[i]\n",
    "        \n",
    "    return {\n",
    "        'intent':{\n",
    "            'intent_type': intent_label,\n",
    "            'score': intent_score,\n",
    "        },\n",
    "        'slot': slot_list,\n",
    "        'text': text,\n",
    "    }\n",
    "# Bug từ đầu tiên (\"thứ 3 brazil đá mấy giờ\"), \"chủ nhật brazil đá mấy giờ\", \"chủ nhật brazil diến ra mấy giờ\"\n",
    "inference(\"xếp hạng của bồ đào nha hiện đang như thế nào vậy maika\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'intent': {'intent_type': 'schedule.nearest', 'score': 0.9995115},\n",
       " 'slot': [{'slot_type': 'team',\n",
       "   'score': 0.8021553,\n",
       "   'slot_text': '',\n",
       "   'start': 0,\n",
       "   'end': 0},\n",
       "  {'slot_type': 'team',\n",
       "   'score': 0.9999466,\n",
       "   'slot_text': 'pháp',\n",
       "   'start': 19,\n",
       "   'end': 23},\n",
       "  {'slot_type': 'team',\n",
       "   'score': 0.99998903,\n",
       "   'slot_text': 'bồ đào nha',\n",
       "   'start': 27,\n",
       "   'end': 37}],\n",
       " 'text': 'maika cho mình hỏi pháp và bồ đào nha khi nào đá với nhau vậy'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'intent': {'intent_type': 'infor.score', 'score': 0.99837565},\n",
       " 'slot': [{'slot_type': 'team',\n",
       "   'score': 0.999966,\n",
       "   'slot_text': 'brazil',\n",
       "   'start': 8,\n",
       "   'end': 14},\n",
       "  {'slot_type': 'team',\n",
       "   'score': 0.9999974,\n",
       "   'slot_text': 'tây ban nha',\n",
       "   'start': 18,\n",
       "   'end': 29}],\n",
       " 'text': 'soi kèo brazil và tây ban nha đi maika'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'intent': {'intent_type': 'infor.score', 'score': 0.9998349},\n",
       " 'slot': [{'slot_type': 'team',\n",
       "   'score': 0.5846616,\n",
       "   'slot_text': '',\n",
       "   'start': 0,\n",
       "   'end': 0},\n",
       "  {'slot_type': 'team',\n",
       "   'score': 0.9995927,\n",
       "   'slot_text': 'anh',\n",
       "   'start': 18,\n",
       "   'end': 21},\n",
       "  {'slot_type': 'team',\n",
       "   'score': 0.9999864,\n",
       "   'slot_text': 'thụy điển',\n",
       "   'start': 25,\n",
       "   'end': 34},\n",
       "  {'slot_type': 'day',\n",
       "   'score': 0.99999714,\n",
       "   'slot_text': 'hôm qua',\n",
       "   'start': 39,\n",
       "   'end': 46}],\n",
       " 'text': 'cho tớ biết tỉ số anh và thụy điển vào hôm qua'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'intent': {'intent_type': 'infor.score', 'score': 0.9649062},\n",
       " 'slot': [{'slot_type': 'team',\n",
       "   'score': 0.6023939,\n",
       "   'slot_text': '',\n",
       "   'start': 0,\n",
       "   'end': 0},\n",
       "  {'slot_type': 'branch',\n",
       "   'score': 0.999985,\n",
       "   'slot_text': 'bảng a',\n",
       "   'start': 0,\n",
       "   'end': 6}],\n",
       " 'text': 'bảng a có kết quả như thế nào rồi'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'intent': {'intent_type': 'result.predict.winner', 'score': 0.88982755},\n",
       " 'slot': [{'slot_type': 'team',\n",
       "   'score': 0.80810153,\n",
       "   'slot_text': '',\n",
       "   'start': 0,\n",
       "   'end': 0},\n",
       "  {'slot_type': 'team',\n",
       "   'score': 0.999948,\n",
       "   'slot_text': 'anh',\n",
       "   'start': 10,\n",
       "   'end': 13},\n",
       "  {'slot_type': 'rank',\n",
       "   'score': 0.9251872,\n",
       "   'slot_text': 'nhất',\n",
       "   'start': 26,\n",
       "   'end': 30}],\n",
       " 'text': 'đội tuyển anh có khả năng nhất không'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'intent': {'intent_type': 'infor.score', 'score': 0.99967086},\n",
       " 'slot': [{'slot_type': 'team',\n",
       "   'score': 0.75083566,\n",
       "   'slot_text': '',\n",
       "   'start': 0,\n",
       "   'end': 0},\n",
       "  {'slot_type': 'team',\n",
       "   'score': 0.9999443,\n",
       "   'slot_text': 'anh',\n",
       "   'start': 8,\n",
       "   'end': 11},\n",
       "  {'slot_type': 'team',\n",
       "   'score': 0.9999559,\n",
       "   'slot_text': 'bỉ',\n",
       "   'start': 15,\n",
       "   'end': 17},\n",
       "  {'slot_type': 'day',\n",
       "   'score': 0.99999857,\n",
       "   'slot_text': 'ngày mai',\n",
       "   'start': 18,\n",
       "   'end': 26}],\n",
       " 'text': 'dự đoán anh và bỉ ngày mai ai thắng'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'intent': {'intent_type': 'schedule', 'score': 0.99887854},\n",
       " 'slot': [{'slot_type': 'branch',\n",
       "   'score': 0.9999962,\n",
       "   'slot_text': 'bảng f',\n",
       "   'start': 8,\n",
       "   'end': 14}],\n",
       " 'text': 'khi nào bảng f đá vậy'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'intent': {'intent_type': 'schedule', 'score': 0.9998155},\n",
       " 'slot': [{'slot_type': 'day',\n",
       "   'score': 0.9999931,\n",
       "   'slot_text': 'ngày mai',\n",
       "   'start': 18,\n",
       "   'end': 26}],\n",
       " 'text': 'xem lịch worldcup ngày mai'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'intent': {'intent_type': 'infor.score', 'score': 0.9997377},\n",
       " 'slot': [{'slot_type': 'team',\n",
       "   'score': 0.82631934,\n",
       "   'slot_text': '',\n",
       "   'start': 0,\n",
       "   'end': 0},\n",
       "  {'slot_type': 'team',\n",
       "   'score': 0.99997425,\n",
       "   'slot_text': 'bỉ',\n",
       "   'start': 17,\n",
       "   'end': 19},\n",
       "  {'slot_type': 'team',\n",
       "   'score': 0.99997866,\n",
       "   'slot_text': 'pháp',\n",
       "   'start': 23,\n",
       "   'end': 27}],\n",
       " 'text': 'kết quả trận đấu bỉ và pháp vào bao nhiêu quả'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "testcase = [\n",
    "    \"maika cho mình hỏi pháp và bồ đào nha khi nào đá với nhau vậy\",\n",
    "    'soi kèo brazil và tây ban nha đi maika',\n",
    "    'cho tớ biết tỉ số anh và thụy điển vào hôm qua',\n",
    "    'bảng a có kết quả như thế nào rồi',\n",
    "    'đội tuyển anh có khả năng nhất không',\n",
    "    'dự đoán anh và bỉ ngày mai ai thắng',\n",
    "    'khi nào bảng f đá vậy',\n",
    "    'xem lịch worldcup ngày mai',\n",
    "    'kết quả trận đấu bỉ và pháp vào bao nhiêu quả'\n",
    "]\n",
    "for i in testcase:\n",
    "    display(inference(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/viet/miniconda3/envs/worldcup/lib/python3.9/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:744: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  scale = torch.sqrt(torch.tensor(query_layer.size(-1), dtype=torch.float) * scale_factor)\n",
      "/home/viet/miniconda3/envs/worldcup/lib/python3.9/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:744: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  scale = torch.sqrt(torch.tensor(query_layer.size(-1), dtype=torch.float) * scale_factor)\n",
      "/home/viet/miniconda3/envs/worldcup/lib/python3.9/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:745: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  attention_scores = torch.bmm(query_layer, key_layer.transpose(-1, -2)) / torch.tensor(\n",
      "/home/viet/miniconda3/envs/worldcup/lib/python3.9/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:136: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  output = input.masked_fill(rmask, torch.tensor(torch.finfo(input.dtype).min))\n",
      "/tmp/ipykernel_434702/1407398840.py:18: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if not (query.size()[-1] == self.hidden_size and context.size()[-1] == self.hidden_size):\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# def Convert_ONNX(model, data_input, logs_dir = \"./logs\"):\n",
    "# Export the model\n",
    "inputs = tokenizer(\"This is a sample\", return_tensors = 'pt',return_offsets_mapping= True)\n",
    "del inputs['token_type_ids']\n",
    "# os.makedirs(logs_dir, exist_ok=True)\n",
    "torch.onnx.export(\n",
    "    worldcup_model,               # model being run\n",
    "    (inputs['input_ids'], inputs['attention_mask']), # model input (or a tuple for multiple inputs)\n",
    "    f=\"worldcup-model.onnx\",   # where to save the model (can be a file or file-like object)\n",
    "    opset_version=13,          # the ONNX version to export the model to\n",
    "    do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "    input_names = ['input_ids', 'attention_mask'],   # the model's input names\n",
    "    output_names = ['intent_logits', 'slot_logits'], # the model's output names\n",
    "    dynamic_axes={'input_ids' : {0 : 'batch_size', 1: 'sequence_len'},    # variable length axes\n",
    "                    'attention_mask' : {0 : 'batch_size', 1: 'sequence_len'},\n",
    "                    'intent_logits' : {0 : 'batch_size', 1: 'intent_labels'},\n",
    "                    'slot_logits' : {0 : 'batch_size', 1: 'sequence_len', 2: 'slot_labels'}}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b6b6957e647cff3590d21b9add6a5ded85b108c596d11fc0746d4da2df84f84b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('worldcup')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b6b6957e647cff3590d21b9add6a5ded85b108c596d11fc0746d4da2df84f84b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
